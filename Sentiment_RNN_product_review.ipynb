{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with an RNN\n",
    "\n",
    "In this notebook, I implemented a recurrent neural network that performs sentiment analysis. \n",
    ">Using an RNN rather than a strictly feedforward network is more accurate since we can include information about the *sequence* of words. \n",
    "\n",
    "Here I'll use a dataset of Amazon baby products reviews, accompanied by product names and rates.\n",
    "\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "The architecture for this network is shown below.\n",
    "\n",
    "<img src=\"assets/network_diagram_product.png\" width=40%>\n",
    "\n",
    ">**First, I'll pass in words to an embedding layer.** We need an embedding layer because we have tens of thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. You can actually train an embedding with the Skip-gram Word2Vec model and use those embeddings as input, here. However, it's good enough to just have an embedding layer and let the network learn a different embedding table on its own. *In this case, the embedding layer is for dimensionality reduction, rather than for learning semantic representations.*\n",
    "\n",
    ">**After input words are passed to an embedding layer, the new embeddings will be passed to LSTM cells.** The LSTM cells will add *recurrent* connections to the network and give us the ability to include information about the *sequence* of words in the product review data. \n",
    "\n",
    ">**Finally, the LSTM outputs will go to a sigmoid output layer.** We're using a sigmoid function because positive and negative = 1 and 0, respectively, and a sigmoid will output predicted, sentiment values between 0-1. \n",
    "\n",
    "We don't care about the sigmoid outputs except for the **very last one**; we can ignore the rest. We'll calculate the loss by comparing the output at the last time step and the training label (pos or neg).\n",
    "\n",
    "\n",
    "#### Outline:\n",
    "\n",
    "* [Load in and visualize the data](#1)\n",
    "* [Data pre-processing](#2)\n",
    "    * [Tokenizing the words](#2_1)\n",
    "    * [Encoding the labels](#2_2)\n",
    "    * [Removing Outliers](#2_3)\n",
    "* [Padding sequences](#3)\n",
    "* [Training, Validation, Test split](#4)\n",
    "* [DataLoaders and Batching](#5)\n",
    "* [Sentiment Network with PyTorch](#6)\n",
    "    * [Instantiate the network](#6_1)\n",
    "    * [Training](#6_2)\n",
    "    * [Testing](#6_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id =\"1\"></a>\n",
    "## Load in and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                           Planetwise Flannel Wipes   \n",
       "1                              Planetwise Wipe Pouch   \n",
       "2                Annas Dream Full Quilt with 2 Shams   \n",
       "3  Stop Pacifier Sucking without tears with Thumb...   \n",
       "4  Stop Pacifier Sucking without tears with Thumb...   \n",
       "\n",
       "                                              review  rating  \n",
       "0  These flannel wipes are OK, but in my opinion ...       3  \n",
       "1  it came early and was not disappointed. i love...       5  \n",
       "2  Very soft and comfortable and warmer than it l...       5  \n",
       "3  This is a product well worth the purchase.  I ...       5  \n",
       "4  All of my kids have cried non-stop when I trie...       5  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "products = pd.read_csv(\"data/amazon_baby.csv\")\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These flannel wipes are OK, but in my opinion not worth keeping.  I also ordered someImse Vimse Cloth Wipes-Ocean Blue-12 countwhich are larger, had a nicer, softer texture and just seemed higher quality.  I use cloth wipes for hands and faces and have been usingThirsties 6 Pack Fab Wipes, Boyfor about 8 months now and need to replace them because they are starting to get rough and have had stink issues for a while that stripping no longer handles.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[\"review\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 183531 entries, 0 to 183530\n",
      "Data columns (total 3 columns):\n",
      "name      183213 non-null object\n",
      "review    182702 non-null object\n",
      "rating    183531 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "products.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## Data pre-processing\n",
    "\n",
    "The first step when building a neural network model is getting your data into the proper form to feed into the network. Since I'm using embedding layers, I'll encode each word with an integer. I'll also want to clean it up a bit.\n",
    "\n",
    "You can see an example of the reviews data above. Here are the processing steps, I'll want to take:\n",
    "> * I'll remove all products without a review or name.\n",
    "* I'll also remove all the reviews whose rate is 3. Because I can't decide 3 as a positive or negative rate.\n",
    "* I'll want to make all the characters lowercase.\n",
    "* I'll get rid of periods and extraneous punctuation.\n",
    "* Then I'll combine all the reviews together into one big string.\n",
    "\n",
    "First, let's remove all punctuation. Then get all the text and split it into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name      318\n",
      "review    829\n",
      "rating      0\n",
      "dtype: int64\n",
      "\n",
      "name      0\n",
      "review    0\n",
      "rating    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove all the products without a review or name\n",
    "print(products.isna().sum())\n",
    "print()\n",
    "products.dropna(axis = 0, inplace = True)\n",
    "print(products.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16705\n"
     ]
    }
   ],
   "source": [
    "# Remove all the reviews whose rate is 3\n",
    "print(len(products[products[\"rating\"] == 3]))\n",
    "products = products[products[\"rating\"]!=3]\n",
    "\n",
    "assert len(products[products[\"rating\"] == 3]) == 0               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index after removing some rows\n",
    "products.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the reviews : 165679\n",
      "Number of unique products : 30629\n"
     ]
    }
   ],
   "source": [
    "# Check out the number of remaining data\n",
    "print(f\"Number of the reviews : {products.shape[0]}\")\n",
    "print(f\"Number of unique products : {len(products['name'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">As you can see we have `165679` reviews for `30629` unique products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "print(punctuation)\n",
    "\n",
    "# Make all the characters lowercase\n",
    "products[\"review\"] = products[\"review\"].str.lower()\n",
    "\n",
    "# Eliminate all the punctuations \n",
    "products[\"review\"] = products[\"review\"].apply(lambda review: ''.join([c for c in review if c not in punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Combine all the reviews together into one big string.\n",
    "reviews = list(products[\"review\"])\n",
    "all_text = \" \".join(reviews)\n",
    "\n",
    "# Create a list of the words\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'came',\n",
       " 'early',\n",
       " 'and',\n",
       " 'was',\n",
       " 'not',\n",
       " 'disappointed',\n",
       " 'i',\n",
       " 'love',\n",
       " 'planet',\n",
       " 'wise',\n",
       " 'bags',\n",
       " 'and',\n",
       " 'now',\n",
       " 'my',\n",
       " 'wipe',\n",
       " 'holder',\n",
       " 'it',\n",
       " 'keps',\n",
       " 'my']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13326115"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"2_1\"></a>\n",
    "### Tokenizing the words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.\n",
    "\n",
    "> * Now I'm going to encode the words with integers. Later I'm going to pad the input vectors with zeros, so the integers **start at 1, not 0**.\n",
    "* Also, I'm going to convert the reviews to integers and store the reviews in a new list called `reviews_ints`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count of each word and sort from most frequent to least\n",
    "vocab_count = Counter(words)\n",
    "vocab_sorted = [vocab for vocab,_ in vocab_count.most_common()]\n",
    "\n",
    "# Build a dictionary that maps words to integers\n",
    "vocab_to_int = {vocab: i+1 for i, vocab in enumerate(vocab_sorted)}\n",
    "\n",
    "# Store the tokenized reviews in reviews_ints\n",
    "reviews_ints = [list(map(lambda c: vocab_to_int[c],r.split())) for r in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  141028\n",
      "\n",
      "Tokenized review: \n",
      " [[3, 254, 1057, 2, 16, 21, 481, 4, 50, 3511, 2460, 360, 2, 79, 10, 754, 679, 3, 48247, 10, 5472, 455, 4140, 2, 117, 21, 467, 245, 100, 3], [28, 148, 2, 196, 2, 829, 73, 3, 48248, 1, 505, 144, 256, 48249, 100, 5, 380, 213, 9, 8, 759, 11, 2817], [8, 7, 6, 66, 58, 205, 1, 221, 4, 17, 21, 218, 328, 506, 45, 8, 2, 3, 7, 6, 1597, 5130, 6734, 5, 2510, 1, 3165, 106, 4, 50, 220, 67, 8, 66, 7, 121, 75, 11178, 10, 86, 52, 12, 225, 2074, 11, 1, 3165, 41, 7, 20, 3538, 11, 845, 2, 94, 48, 55, 7070, 4, 50, 1, 8344, 1, 3186, 12, 1, 93, 2, 1, 3819, 6734, 11, 8, 1945], [42, 11, 10, 192, 17, 2672, 5017, 29, 4, 206, 5, 11179, 47, 99, 169, 665, 295, 4, 218, 48250, 5, 94, 3165, 7070, 6735, 3, 7, 81, 54, 133, 5, 137, 14, 82, 192, 5, 901, 47, 5, 1176, 263, 169, 665, 7, 212, 2, 338, 47, 286, 57, 5258, 7, 6, 386, 123, 798, 2, 6, 32, 249, 9, 893, 493, 23, 61, 774, 47, 3717, 198, 48251, 9, 8, 798, 23, 42, 1408]]\n"
     ]
    }
   ],
   "source": [
    "# Stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))\n",
    "print()\n",
    "\n",
    "# Print tokens in first review\n",
    "print('Tokenized review: \\n', reviews_ints[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"2_2\"></a>\n",
    "### Encoding the labels\n",
    "\n",
    "\n",
    "Our labels are rating to movies. To use these labels in our network, we need to convert them to 0 and 1.\n",
    "I'm going to convert ratings below 3 to `0(negative)` and others to `1(positive)`.\n",
    "\n",
    "**Note that I removed all the reviews whose ranking was equal 3.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1=positive (rating>3), 0=negative (rating<3) label conversion\n",
    "encoded_labels  = [int(rate> 3) for rate in products[\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_labels[108:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 5, 5, 5, 5, 5, 4, 4, 1, 4, 5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(products[\"rating\"][108:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"2_3\"></a>\n",
    "### Removing Outliers\n",
    "\n",
    "As an additional pre-processing step, I want to make sure that the reviews are in good shape for standard processing. That is, the network will expect a standard input text size, and so, I'll want to shape the reviews into a specific length. I'll approach this task in two main steps:\n",
    "\n",
    "1. Getting rid of extremely long or short reviews; the outliers\n",
    "2. Padding/truncating the remaining data so that we have reviews of the same length.\n",
    "\n",
    "<img src=\"assets/outliers_padding_ex.png\" width=40%>\n",
    "\n",
    "Before I pad the review text, I should check for reviews of extremely short or long lengths; outliers that may mess with the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2699\n"
     ]
    }
   ],
   "source": [
    "# outlier review stats\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(f\"Zero-length reviews: {review_lens[0]}\")\n",
    "print(f\"Maximum review length: {max(review_lens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">So, a couple issues here. We seem to have one review with zero length. And, the maximum review length is way too many steps for the RNN. I'll have to remove any super short reviews and truncate super long reviews. This removes outliers and should allow the model to train more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  165679\n",
      "Number of reviews after removing outliers:  165678\n"
     ]
    }
   ],
   "source": [
    "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "\n",
    "# Get the indices of reviews with length bigger than 0\n",
    "non_zero_ind = [i for i, review in enumerate(reviews_ints) if len(review)>0]\n",
    "\n",
    "# Remove any reviews/labels with zero length from the reviews_ints list.\n",
    "reviews_ints = np.array([reviews_ints[i] for i in non_zero_ind])\n",
    "encoded_labels = np.array([encoded_labels[i] for i in non_zero_ind])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id =\"3\"></a>\n",
    "## Padding sequences\n",
    "\n",
    "To deal with both short and very long reviews, I'll pad or truncate all the reviews to a specific length. For reviews shorter than some `seq_length`, I'll pad with 0s. For reviews longer than `seq_length`, I can truncate them to the first `seq_length` words. A good `seq_length`, in this case, is 200.\n",
    "\n",
    "As a small example, if the `seq_length=10` and an input review is: \n",
    "```\n",
    "[117, 18, 128]\n",
    "```\n",
    "The resultant, padded sequence should be: \n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
    "```\n",
    "\n",
    ">The final `features` array is going to be a 2D array, with as many rows as there are reviews, and as many columns as the specified `seq_length`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "    \n",
    "    for i, review in enumerate(reviews_ints):\n",
    "        features[i, -len(review):] = np.array(review)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0    29     1  3165]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [ 1315  2188   316   967    68    52  1021  3840    24    41]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    6  1188     9  1087 12208     2     6  1716    36   303]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     4    69     8 14178     9   592     9    10]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [  201    68  1310     6   277   156  4097     8     7     1]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     4    50     8  4097]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "## test statements \n",
    "assert len(features)==len(reviews_ints), \"Features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[:30,100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"4\"></a>\n",
    "## Training, Validation, Test split\n",
    "\n",
    "With the data in nice shape, I'll split it into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Feature Shapes:      Label shape\n",
      "Train set:           (149110, 200)        (149110,)\n",
      "Validation set:      (8284, 200)          (8284,)\n",
      "Test set:            (8284, 200)          (8284,)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.9\n",
    "\n",
    "\n",
    "# Split data into training, validation, and test data (features and labels, x and y)\n",
    "train_ind = int(split_frac*len(features))\n",
    "train_x, train_y = features[:train_ind], encoded_labels[:train_ind]\n",
    "others_x, others_y = features[train_ind:], encoded_labels[train_ind:]\n",
    "\n",
    "valid_ind = int(0.5*len(others_x))\n",
    "valid_x, valid_y = others_x[:valid_ind], others_y[:valid_ind]\n",
    "test_x, test_y = others_x[valid_ind:], others_y[valid_ind:]\n",
    "\n",
    "# Print out the shapes of resultant feature data\n",
    "print(\"\".ljust(20), \"Feature Shapes:\".ljust(20), \"Label shape\" )\n",
    "print(\"Train set:\".ljust(20), f\"{train_x.shape}\".ljust(20), train_y.shape)\n",
    "print(\"Validation set:\".ljust(20), f\"{valid_x.shape}\".ljust(20), valid_y.shape)\n",
    "print(\"Test set:\".ljust(20), f\"{test_x.shape}\".ljust(20), test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id= \"5\"></a>\n",
    "## DataLoaders and Batching\n",
    "\n",
    "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
    "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
    "2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
    "\n",
    "This is an alternative to creating a generator function for batching our data into full batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([64, 200])\n",
      "Sample input: \n",
      " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.8100e+02,\n",
      "          5.6780e+03,  1.7580e+03],\n",
      "        [ 1.3510e+03,  3.7270e+03,  6.6900e+02,  ...,  7.0900e+02,\n",
      "          2.0000e+00,  1.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.0500e+02,\n",
      "          1.6400e+02,  1.1870e+03],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.0000e+00,\n",
      "          5.4300e+02,  6.8000e+01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "          1.4180e+03,  1.2500e+02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0100e+02,\n",
      "          1.0520e+03,  6.3000e+01]])\n",
      "\n",
      "Sample label size:  torch.Size([64])\n",
      "Sample label: \n",
      " tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  0,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  0,  1,  1,  0,  1,  1,  0,  1,  1,  1,  1,  1,  0,\n",
      "         0,  1,  1,  1,  1,  1,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "train_iter = iter(train_loader)\n",
    "sample_reviews, sample_labels = train_iter.next()\n",
    "\n",
    "print('Sample input size: ', sample_reviews.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_reviews)\n",
    "print()\n",
    "print('Sample label size: ', sample_labels.size()) # batch_size\n",
    "print('Sample label: \\n', sample_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"6\"></a>\n",
    "# Sentiment Network with PyTorch\n",
    "\n",
    "The network architecture : \n",
    "\n",
    "<img src=\"assets/network_diagram_product.png\" width=40%>\n",
    "\n",
    "The layers are as follows:\n",
    "1. An [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) that converts our word tokens (integers) into embeddings of a specific size.\n",
    "2. An [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) defined by a hidden_state size and number of layers\n",
    "3. A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n",
    "4. A sigmoid activation layer which turns all outputs into a value 0-1; return **only the last sigmoid output** as the output of this network.\n",
    "\n",
    "#### The Embedding Layer\n",
    "\n",
    "We need to add an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) because there are 141000+ words in the vocabulary. It is massively inefficient to one-hot encode that many classes. So, instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. You could train an embedding layer using Word2Vec, then load it here. But, it's fine to just make a new layer, using it for only dimensionality reduction, and let the network learn the weights.\n",
    "\n",
    "\n",
    "#### The LSTM Layer(s)\n",
    "\n",
    "I'll create an [LSTM](https://pytorch.org/docs/stable/nn.html#lstm) to use in the recurrent network, which takes in an input_size, a hidden_dim, a number of layers, a dropout probability (for dropout between multiple layers), and a batch_first parameter.\n",
    "\n",
    "Most of the time, the network will have better performance with more layers; between 2-3. Adding more layers allows the network to learn really complex relationships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(\"Training on GPU\" if train_on_gpu else \"Training on CPU. No GPU is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # layers\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, self.n_layers, \n",
    "                            dropout = drop_prob, \n",
    "                            batch_first = True)\n",
    "        self.output = nn.Linear(self.hidden_dim, self.output_size)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embed(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.output(out)\n",
    "        sig_out = nn.functional.sigmoid(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.contiguous().view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initializes hidden state\"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        # Initialized to zero, for hidden state and cell state of LSTM\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else :\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"6_1\"></a>\n",
    "## Instantiate the network\n",
    "\n",
    "Here, I'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of the embeddings.\n",
    "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise.\n",
    "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embed): Embedding(141029, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (output): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int) +1\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 3\n",
    "\n",
    "net = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =\"6_2\"></a>\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "lr = 0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3... Step: 100... Loss: 0.337720... Val Loss: 0.319798\n",
      "Epoch: 1/3... Step: 200... Loss: 0.306408... Val Loss: 0.266114\n",
      "Epoch: 1/3... Step: 300... Loss: 0.319464... Val Loss: 0.332644\n",
      "Epoch: 1/3... Step: 400... Loss: 0.247424... Val Loss: 0.242880\n",
      "Epoch: 1/3... Step: 500... Loss: 0.351944... Val Loss: 0.317771\n",
      "Epoch: 1/3... Step: 600... Loss: 0.298553... Val Loss: 0.212778\n",
      "Epoch: 1/3... Step: 700... Loss: 0.195063... Val Loss: 0.201189\n",
      "Epoch: 1/3... Step: 800... Loss: 0.186409... Val Loss: 0.191201\n",
      "Epoch: 1/3... Step: 900... Loss: 0.213330... Val Loss: 0.184585\n",
      "Epoch: 1/3... Step: 1000... Loss: 0.123002... Val Loss: 0.211778\n",
      "Epoch: 1/3... Step: 1100... Loss: 0.246231... Val Loss: 0.174623\n",
      "Epoch: 1/3... Step: 1200... Loss: 0.185962... Val Loss: 0.166628\n",
      "Epoch: 1/3... Step: 1300... Loss: 0.096090... Val Loss: 0.164244\n",
      "Epoch: 1/3... Step: 1400... Loss: 0.211272... Val Loss: 0.163486\n",
      "Epoch: 1/3... Step: 1500... Loss: 0.159955... Val Loss: 0.158895\n",
      "Epoch: 1/3... Step: 1600... Loss: 0.245304... Val Loss: 0.150352\n",
      "Epoch: 1/3... Step: 1700... Loss: 0.120182... Val Loss: 0.160848\n",
      "Epoch: 1/3... Step: 1800... Loss: 0.099642... Val Loss: 0.142467\n",
      "Epoch: 1/3... Step: 1900... Loss: 0.158070... Val Loss: 0.147063\n",
      "Epoch: 1/3... Step: 2000... Loss: 0.103306... Val Loss: 0.140329\n",
      "Epoch: 1/3... Step: 2100... Loss: 0.185944... Val Loss: 0.136391\n",
      "Epoch: 1/3... Step: 2200... Loss: 0.195493... Val Loss: 0.134132\n",
      "Epoch: 1/3... Step: 2300... Loss: 0.261640... Val Loss: 0.139043\n",
      "Epoch: 2/3... Step: 2400... Loss: 0.117398... Val Loss: 0.137929\n",
      "Epoch: 2/3... Step: 2500... Loss: 0.177066... Val Loss: 0.146265\n",
      "Epoch: 2/3... Step: 2600... Loss: 0.205827... Val Loss: 0.131653\n",
      "Epoch: 2/3... Step: 2700... Loss: 0.117518... Val Loss: 0.134908\n",
      "Epoch: 2/3... Step: 2800... Loss: 0.094222... Val Loss: 0.138769\n",
      "Epoch: 2/3... Step: 2900... Loss: 0.056817... Val Loss: 0.132531\n",
      "Epoch: 2/3... Step: 3000... Loss: 0.053554... Val Loss: 0.126194\n",
      "Epoch: 2/3... Step: 3100... Loss: 0.110279... Val Loss: 0.139043\n",
      "Epoch: 2/3... Step: 3200... Loss: 0.113335... Val Loss: 0.130149\n",
      "Epoch: 2/3... Step: 3300... Loss: 0.105142... Val Loss: 0.139618\n",
      "Epoch: 2/3... Step: 3400... Loss: 0.180733... Val Loss: 0.133340\n",
      "Epoch: 2/3... Step: 3500... Loss: 0.106427... Val Loss: 0.126059\n",
      "Epoch: 2/3... Step: 3600... Loss: 0.153785... Val Loss: 0.131294\n",
      "Epoch: 2/3... Step: 3700... Loss: 0.169024... Val Loss: 0.126833\n",
      "Epoch: 2/3... Step: 3800... Loss: 0.091304... Val Loss: 0.129434\n",
      "Epoch: 2/3... Step: 3900... Loss: 0.104843... Val Loss: 0.144760\n",
      "Epoch: 2/3... Step: 4000... Loss: 0.089251... Val Loss: 0.125577\n",
      "Epoch: 2/3... Step: 4100... Loss: 0.231467... Val Loss: 0.136508\n",
      "Epoch: 2/3... Step: 4200... Loss: 0.063451... Val Loss: 0.120762\n",
      "Epoch: 2/3... Step: 4300... Loss: 0.102607... Val Loss: 0.122020\n",
      "Epoch: 2/3... Step: 4400... Loss: 0.236811... Val Loss: 0.138620\n",
      "Epoch: 2/3... Step: 4500... Loss: 0.084289... Val Loss: 0.121087\n",
      "Epoch: 2/3... Step: 4600... Loss: 0.118766... Val Loss: 0.124572\n",
      "Epoch: 3/3... Step: 4700... Loss: 0.063353... Val Loss: 0.122978\n",
      "Epoch: 3/3... Step: 4800... Loss: 0.078291... Val Loss: 0.131572\n",
      "Epoch: 3/3... Step: 4900... Loss: 0.033171... Val Loss: 0.124554\n",
      "Epoch: 3/3... Step: 5000... Loss: 0.057643... Val Loss: 0.126144\n",
      "Epoch: 3/3... Step: 5100... Loss: 0.097521... Val Loss: 0.143206\n",
      "Epoch: 3/3... Step: 5200... Loss: 0.041916... Val Loss: 0.127003\n",
      "Epoch: 3/3... Step: 5300... Loss: 0.131288... Val Loss: 0.163249\n",
      "Epoch: 3/3... Step: 5400... Loss: 0.036091... Val Loss: 0.161792\n",
      "Epoch: 3/3... Step: 5500... Loss: 0.055660... Val Loss: 0.133602\n",
      "Epoch: 3/3... Step: 5600... Loss: 0.077550... Val Loss: 0.124725\n",
      "Epoch: 3/3... Step: 5700... Loss: 0.045449... Val Loss: 0.129908\n",
      "Epoch: 3/3... Step: 5800... Loss: 0.068619... Val Loss: 0.131429\n",
      "Epoch: 3/3... Step: 5900... Loss: 0.030151... Val Loss: 0.136518\n",
      "Epoch: 3/3... Step: 6000... Loss: 0.104334... Val Loss: 0.127938\n",
      "Epoch: 3/3... Step: 6100... Loss: 0.016013... Val Loss: 0.135689\n",
      "Epoch: 3/3... Step: 6200... Loss: 0.042017... Val Loss: 0.127446\n",
      "Epoch: 3/3... Step: 6300... Loss: 0.120390... Val Loss: 0.125398\n",
      "Epoch: 3/3... Step: 6400... Loss: 0.153794... Val Loss: 0.136924\n",
      "Epoch: 3/3... Step: 6500... Loss: 0.043895... Val Loss: 0.131436\n",
      "Epoch: 3/3... Step: 6600... Loss: 0.103782... Val Loss: 0.126607\n",
      "Epoch: 3/3... Step: 6700... Loss: 0.046361... Val Loss: 0.129660\n",
      "Epoch: 3/3... Step: 6800... Loss: 0.088018... Val Loss: 0.129565\n",
      "Epoch: 3/3... Step: 6900... Loss: 0.044832... Val Loss: 0.131084\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "min_val_loss = np.Inf # Keep track of best version of the model\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "\n",
    "if train_on_gpu:\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "for e in range(epochs):\n",
    "    hidden = net.init_hidden(batch_size)\n",
    "    \n",
    "    for in_reviews, labels in train_loader:\n",
    "        counter += 1\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            in_reviews, labels = in_reviews.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history            \n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        # Zero accumulated gradients               \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, hidden = net(in_reviews, hidden)\n",
    "        \n",
    "        # Calculate the loss and perform backprop        \n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_hidden = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            \n",
    "            for val_reviews, val_labels in valid_loader:\n",
    "                if train_on_gpu:\n",
    "                    val_reviews, val_labels = val_reviews.cuda(), val_labels.cuda()\n",
    "                    \n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "                \n",
    "                val_output, val_hidden = net(val_reviews, val_hidden)\n",
    "                \n",
    "                val_loss = criterion(val_output.squeeze(), val_labels.float())\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            val_loss_mean_batch = np.mean(val_losses)\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(val_loss_mean_batch))\n",
    "            \n",
    "            # Save the model with the best validation loss\n",
    "            if val_loss_mean_batch < min_val_loss:\n",
    "                min_val_loss = val_loss_mean_batch\n",
    "                torch.save(net.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"6_3\"></a>\n",
    "## Testing\n",
    "\n",
    "There are a few ways to test the network.\n",
    "\n",
    "* **Test data performance:** First, I'll see how the trained model performs on all of the defined test_data, above. I'll calculate the average loss and accuracy over the test data.\n",
    "\n",
    "* **Inference on user-generated data:** Second, I'll see if I can input just one example review at a time (without a label), and see what the trained model predicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model for testing\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "net.load_state_dict(torch.load('model.pt', map_location=device ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.124\n",
      "Test accuracy: 0.951\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # Get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # Calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # Convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # Compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# Avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# Accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on a test review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative test review\n",
    "test_review_neg = \"It was nothing special, and to be honest, I want my money back!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200):\n",
    "    ''' Prints out whether a given review is predicted to be \n",
    "        positive or negative in sentiment, using a trained model.\n",
    "        \n",
    "        params:\n",
    "        net - A trained net \n",
    "        test_review - a review made of normal text and punctuation\n",
    "        sequence_length - the padded length of a review\n",
    "        '''\n",
    "    #### Preprocessing steps ####\n",
    "    \n",
    "    test_review = test_review.lower()\n",
    "    \n",
    "    # Get rid of punctuation\n",
    "    test_review = \"\".join([c for c in test_review if c not in punctuation])\n",
    "    \n",
    "    # Splitting to words\n",
    "    test_review = test_review.split()\n",
    "    \n",
    "    # Tokenize the review\n",
    "    test_review = [[vocab_to_int[c] for c in test_review if c in vocab_to_int.keys()]]\n",
    "\n",
    "    # Pad tokenized sequence\n",
    "    test_review = pad_features(test_review, seq_length = sequence_length)\n",
    "    \n",
    "    #### Predict ####\n",
    "    \n",
    "    # Conver to Tensor\n",
    "    test_review = torch.from_numpy(test_review)\n",
    "    \n",
    "    hidden = net.init_hidden(test_review.size(0))\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        test_review = test_review.cuda()\n",
    "        \n",
    "    output, hidden = net(test_review, hidden)\n",
    "    pred = torch.round(output.squeeze())\n",
    "    \n",
    "    # print custom response based on whether test_review is pos/neg\n",
    "    print(f\"The review is predicted as {'POSITIVE' if pred.item()==1 else 'NEGATIVE'}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive test review\n",
    "test_review_pos = \"This product was better than I'd expected. I loved it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review is predicted as NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# call function\n",
    "# try negative and positive reviews!\n",
    "seq_length=200\n",
    "predict(net, test_review_neg, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review is predicted as POSITIVE\n"
     ]
    }
   ],
   "source": [
    "predict(net, test_review_pos, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
